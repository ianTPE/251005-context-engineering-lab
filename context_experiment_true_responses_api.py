"""
Context Engineering Experiment - TRUE Responses API
====================================================
‰ΩøÁî®ÁúüÊ≠£ÁöÑ OpenAI Responses API (client.responses.create)

ÈáçË¶ÅÊõ¥Ê≠£Ôºö
- Responses API ‰ΩøÁî® POST /v1/responses
- Ëº∏ÂÖ•ÁÇ∫ 'input' (Â≠ó‰∏≤) ËÄåÈùû 'messages' (Èô£Âàó)
- Ëº∏Âá∫ÁÇ∫ 'output_text' ËÄåÈùû 'choices[0].message.content'
- ÊîØÊè¥ÂéüÁîü MCP Â∑•ÂÖ∑Âíå web_search
"""

import json
import os
from datetime import datetime
from openai import OpenAI

# Check for API key
if not os.getenv("OPENAI_API_KEY"):
    print("‚ùå ERROR: OPENAI_API_KEY environment variable not set!")
    print("\nPlease set your OpenAI API key:")
    print("  Windows (PowerShell): $env:OPENAI_API_KEY='your-key-here'")
    exit(1)

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Test sentences - 5 longer, more realistic reviews
TESTS = [
    "ÈÄôÊîØËÄ≥Ê©üÈü≥Ë≥™‰∏çÈåØÔºå‰ΩÜËóçÁâôÂ∏∏Â∏∏Êñ∑Á∑ö„ÄÇ",

    "The keyboard feels great, but the battery dies too fast.",
    
    "Áõ∏Ê©üÁï´Ë≥™ÂæàÊ£íÔºåÂèØÊòØÂ§úÊãçÂ∞çÁÑ¶ÂæàÊÖ¢„ÄÇ"

    "ÊàëÊúÄËøëË≤∑‰∫ÜÈÄôÊ¨æÁÑ°Á∑öËÄ≥Ê©üÔºåÊï¥È´î‰æÜË™™Èü≥Ë≥™Ë°®ÁèæÁõ∏Áï∂Âá∫Ëâ≤Ôºå‰ΩéÈü≥Ê∏æÂéö„ÄÅÈ´òÈü≥Ê∏ÖÊô∞„ÄÇ‰∏çÈÅé‰ΩøÁî®‰∫ÜÂÖ©ÂÄãÁ¶ÆÊãúÂæåÁôºÁèæÔºåËóçÁâôÈÄ£Á∑öÁ∂ìÂ∏∏ÊúÉÁ™ÅÁÑ∂Êñ∑ÊéâÔºåÂ∞§ÂÖ∂ÊòØÂú®‰∫∫Â§öÁöÑÂú∞ÊñπÊõ¥ÊòéÈ°ØÔºåÈúÄË¶ÅÈáçÊñ∞ÈÖçÂ∞çÊâçËÉΩ‰ΩøÁî®ÔºåÈÄôÈªûÁúüÁöÑÂæàÂõ∞Êìæ„ÄÇ",
    
    "The mechanical keyboard I purchased has excellent build quality with a satisfying tactile feedback that makes typing a pleasure. However, I'm quite disappointed with the battery life - it only lasts about 3-4 days with the RGB lighting on, which is far shorter than the advertised 2 weeks. I find myself charging it constantly.",
    
    "ÈÄôÂè∞Áõ∏Ê©üÁöÑÁï´Ë≥™ÁúüÁöÑÊ≤íË©±Ë™™ÔºåÊó•ÊãçÁöÑÁÖßÁâáËâ≤ÂΩ©ÈÆÆË±î„ÄÅÁ¥∞ÁØÄË±êÂØåÔºåÂÆåÂÖ®ÈÅîÂà∞Â∞àÊ•≠Ê∞¥Ê∫ñ„ÄÇ‰ΩÜÊòØ‰∏ÄÂà∞Êôö‰∏äÊàñÂÖâÁ∑ö‰∏çË∂≥ÁöÑÁí∞Â¢ÉÔºåÂ∞çÁÑ¶ÈÄüÂ∫¶Â∞±ËÆäÂæóË∂ÖÁ¥öÊÖ¢ÔºåÂ∏∏Â∏∏Ë¶ÅÂ∞çÂ•ΩÂπæÊ¨°ÊâçËÉΩÊàêÂäüÔºåÊãçÂ§úÊôØÊàñÂÆ§ÂÖßÁÖßÁâáÊôÇÂæà‰∏çÊñπ‰æøÔºåÂ∏åÊúõÊú™‰æÜÈüåÈ´îÊõ¥Êñ∞ËÉΩÊîπÂñÑÈÄôÂÄãÂïèÈ°å„ÄÇ",
    
    "I've been using this wireless mouse for gaming and productivity work for the past month. The ergonomic design is comfortable for long sessions, and the precision is excellent for both gaming and design work. The only downside is that the left click button has started developing a double-click issue, which is frustrating during important tasks.",
    
    "ÈÄôÊ¨æÊô∫ÊÖßÊâãÈå∂ÁöÑËû¢ÂπïÈ°ØÁ§∫ÊïàÊûúÂæàÊ£íÔºåÂú®ÈôΩÂÖâ‰∏ã‰πüËÉΩÊ∏ÖÊ•öÁúãË¶ãÔºåËÄå‰∏îÈÅãÂãïËøΩËπ§ÂäüËÉΩÂæàÊ∫ñÁ¢∫„ÄÇÂèØÊòØÁ∫åËà™ÂäõÁúüÁöÑËÆì‰∫∫Â§±ÊúõÔºåÂÆòÊñπË™™ÂèØ‰ª•Áî®5Â§©Ôºå‰ΩÜÂØ¶Èöõ‰∏äÈñãÂïüÊâÄÊúâÂäüËÉΩÂæåÔºåÂ§ßÊ¶Ç2Â§©Â∞±Ë¶ÅÂÖÖÈõª‰∫Ü„ÄÇÂè¶Â§ñÂÖÖÈõªÈÄüÂ∫¶‰πüÂæàÊÖ¢ÔºåË¶ÅÂÖÖÊªøÈõªÈúÄË¶ÅÂ∞áËøë3Â∞èÊôÇÔºåÂ∞çÊñºÁ∂ìÂ∏∏Â§ñÂá∫ÁöÑ‰∫∫‰æÜË™™Âæà‰∏çÊñπ‰æø„ÄÇ"
]

# ============================================================================
# Responses API ÁöÑ Context Ë®≠Ë®à
# ============================================================================
# Ê≥®ÊÑèÔºöResponses API ‰ΩøÁî®ÂñÆ‰∏Ä 'input' Â≠ó‰∏≤ÔºåËÄåÈùû messages Èô£Âàó
# Few-shot ÂøÖÈ†àÁõ¥Êé•Âú® input ‰∏≠‰ª•ÊñáÂ≠óÂΩ¢ÂºèÊèê‰æõ

def build_context_a_input(user_sentence):
    """Context A: Baseline"""
    return f"""Extract sentiment (positive/neutral/negative), product, and issue from this sentence.
Return as JSON.

Sentence: {user_sentence}"""


def build_context_b_input(user_sentence):
    """Context B: Rules-based"""
    return f"""Task: Extract fields from the sentence.
Return ONLY a JSON object with these exact keys: sentiment, product, issue.

Rules:
- sentiment must be one of: positive, neutral, negative
- If product is not explicit, infer the most likely product noun (e.g., 'headphones', 'keyboard')
- issue should describe the problem mentioned, or be empty string if none
- Return ONLY valid JSON, no comments, no extra text, no markdown code blocks
- Use lowercase English for all field values

Sentence: {user_sentence}"""


def build_context_c_input(user_sentence):
    """Context C: Few-shot (text-based, Âõ†ÁÇ∫ Responses API Âè™Êé•ÂèóÂñÆ‰∏Ä input Â≠ó‰∏≤)"""
    return f"""You are a product review analyzer. Extract sentiment, product, and issue from reviews.

Rules:
- sentiment: must be "positive", "neutral", or "negative"
- product: infer the product type in lowercase English
- issue: describe the problem, or empty string if none
- Return ONLY valid JSON with keys: sentiment, product, issue
- No markdown, no extra text

Examples:

Example 1:
Input: "ÈÄôÂè∞Á≠ÜÈõªËû¢ÂπïÂæà‰∫ÆÔºå‰ΩÜÊòØÊï£ÁÜ±ÂæàÂêµ„ÄÇ"
Output: {{"sentiment": "negative", "product": "laptop", "issue": "noisy cooling"}}

Example 2:
Input: "These earbuds are comfortable and the mic is clear."
Output: {{"sentiment": "positive", "product": "earbuds", "issue": ""}}

Example 3:
Input: "The mouse is lightweight but clicks feel mushy."
Output: {{"sentiment": "negative", "product": "mouse", "issue": "mushy clicks"}}

Now analyze this sentence:
Input: "{user_sentence}"
Output:"""


# ============================================================================
# Responses API Calling Function
# ============================================================================

def call_responses_api(input_text, model="gpt-4o"):
    """
    ‰ΩøÁî®ÁúüÊ≠£ÁöÑ Responses API
    
    Ê≥®ÊÑèÔºö
    1. ‰ΩøÁî® client.responses.create() ËÄåÈùû client.chat.completions.create()
    2. ÂèÉÊï∏ÊòØ 'input' ËÄåÈùû 'messages'
    3. ÂõûÂÇ≥ÊòØ response.output_text ËÄåÈùû response.choices[0].message.content
    """
    try:
        response = client.responses.create(
            model=model,
            input=input_text
        )
        return response.output_text
    except AttributeError as e:
        # Â¶ÇÊûú SDK ÁâàÊú¨‰∏çÊîØÊè¥ responses.createÔºåÊèê‰æõÂèãÂñÑÈåØË™§Ë®äÊÅØ
        return f"ERROR: Your OpenAI SDK version doesn't support responses.create(). Please upgrade: pip install --upgrade openai"
    except Exception as e:
        return f"ERROR: {str(e)}"


# ============================================================================
# Scoring and Evaluation (Âêå‰πãÂâç)
# ============================================================================

def clean_json_output(text):
    """Extract JSON from text that might contain markdown blocks"""
    if "```" in text:
        lines = text.split("\n")
        json_lines = []
        in_code_block = False
        for line in lines:
            if line.strip().startswith("```"):
                in_code_block = not in_code_block
                continue
            if in_code_block:
                json_lines.append(line)
        text = "\n".join(json_lines)
    return text.strip()


def score_json(output_text):
    """Score output based on schema compliance"""
    try:
        cleaned = clean_json_output(output_text)
        obj = json.loads(cleaned)
        
        required_keys = {"sentiment", "product", "issue"}
        keys_ok = set(obj.keys()) == required_keys
        
        valid_sentiments = {"positive", "neutral", "negative"}
        sentiment_ok = obj.get("sentiment", "").lower() in valid_sentiments
        
        product_ok = isinstance(obj.get("product"), str) and len(obj.get("product", "")) > 0
        issue_ok = "issue" in obj and isinstance(obj.get("issue"), str)
        
        if keys_ok and sentiment_ok and product_ok and issue_ok:
            return 1, obj, None
        else:
            errors = []
            if not keys_ok: errors.append(f"wrong_keys: {set(obj.keys())}")
            if not sentiment_ok: errors.append(f"invalid_sentiment: {obj.get('sentiment')}")
            if not product_ok: errors.append("empty_or_invalid_product")
            if not issue_ok: errors.append("missing_or_invalid_issue")
            return 0, obj, ", ".join(errors)
            
    except json.JSONDecodeError as e:
        return 0, None, f"JSON parse error: {str(e)}"
    except Exception as e:
        return 0, None, f"Unexpected error: {str(e)}"


def eval_context(tag, input_builder, verbose=True):
    """Evaluate a context strategy using Responses API"""
    if verbose:
        print(f"\n{'='*70}")
        print(f"  {tag}")
        print(f"{'='*70}\n")
    
    results = []
    total_score = 0
    
    for i, test_sentence in enumerate(TESTS, 1):
        # Build input using the provided builder function
        input_text = input_builder(test_sentence)
        
        # Show input structure for Context C (few-shot)
        if verbose and "Few-shot" in tag and i == 1:
            print(f"üì® Input structure:")
            print(f"   Using text-based few-shot (Responses API limitation)")
            print(f"   Input length: {len(input_text)} chars\n")
        
        # Call Responses API
        output = call_responses_api(input_text)
        
        # Check for SDK error
        if output.startswith("ERROR: Your OpenAI SDK"):
            print(f"\n‚ö†Ô∏è  {output}")
            print("\nüí° Note: Responses API is available but may require SDK version >= 1.50.0")
            print("   This experiment will show you what the code SHOULD look like.")
            return None
        
        # Score output
        score, parsed, error = score_json(output)
        total_score += score
        
        # Store result
        result = {
            "test_id": i,
            "input": test_sentence,
            "output": output,
            "parsed": parsed,
            "score": score,
            "error": error
        }
        results.append(result)
        
        # Print if verbose
        if verbose:
            print(f"Test {i}: {test_sentence}")
            print(f"Output: {output}")
            print(f"Parsed: {json.dumps(parsed, ensure_ascii=False) if parsed else 'FAILED'}")
            print(f"Score: {score}/1 {f'({error})' if error else '‚úì'}")
            print()
    
    if verbose:
        print(f"[{tag}] Total Score: {total_score}/{len(TESTS)}")
        print(f"Success Rate: {total_score/len(TESTS)*100:.1f}%")
    
    return {
        "tag": tag,
        "total_score": total_score,
        "max_score": len(TESTS),
        "success_rate": total_score / len(TESTS),
        "results": results
    }


def run_experiment():
    """Run the complete A/B/C experiment with TRUE Responses API"""
    print("\n" + "="*70)
    print("  CONTEXT ENGINEERING EXPERIMENT")
    print("  Using TRUE OpenAI Responses API (client.responses.create)")
    print("="*70)
    print("\nüí° Key Differences from Chat Completions:")
    print("   - Uses 'input' (string) instead of 'messages' (array)")
    print("   - Returns 'output_text' instead of 'choices[0].message.content'")
    print("   - Supports native MCP tools and web_search")
    print("   - Represents OpenAI's future direction\n")
    
    # Run all three context versions
    results_a = eval_context(
        "A: Baseline (minimal instruction)",
        build_context_a_input
    )
    
    # If SDK error, exit gracefully
    if results_a is None:
        return
    
    results_b = eval_context(
        "B: Rules-based (strict format)",
        build_context_b_input
    )
    
    results_c = eval_context(
        "C: Few-shot (text-based, due to Responses API design)",
        build_context_c_input
    )
    
    # Summary comparison
    print("\n" + "="*70)
    print("  SUMMARY COMPARISON")
    print("="*70 + "\n")
    
    comparison = [
        ("Context A (Baseline)", results_a["success_rate"]),
        ("Context B (Rules)", results_b["success_rate"]),
        ("Context C (Few-shot)", results_c["success_rate"])
    ]
    
    for name, rate in comparison:
        bar = "‚ñà" * int(rate * 20)
        print(f"{name:30s} {bar:20s} {rate*100:5.1f}%")
    
    print("\n" + "-"*70)
    print("Key Findings:")
    print("-"*70)
    
    if results_c["success_rate"] > results_b["success_rate"]:
        print("‚úì Few-shot examples improved consistency over rules alone")
    if results_b["success_rate"] > results_a["success_rate"]:
        print("‚úì Explicit rules improved reliability over baseline")
    if results_c["success_rate"] == 1.0:
        print("‚úì Few-shot context achieved 100% success rate!")
    elif results_c["success_rate"] < 1.0:
        print(f"‚ö† Few-shot still has {(1-results_c['success_rate'])*100:.0f}% failure rate")
    
    print("\n" + "-"*70)
    print("TRUE Responses API Characteristics:")
    print("-"*70)
    print("1. ‚úÖ Simpler API surface (single 'input' parameter)")
    print("2. ‚úÖ Built-in tool support (MCP, web_search)")
    print("3. ‚úÖ Optimized for agentic workflows")
    print("4. ‚ö†Ô∏è  Few-shot must be text-based (no message array)")
    print("5. ‚úÖ Future-proof (OpenAI's recommended direction)")
    
    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"experiment_results_true_responses_api_{timestamp}.json"
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump({
            "timestamp": timestamp,
            "api_version": "TRUE Responses API (POST /v1/responses)",
            "test_sentences": TESTS,
            "results": {
                "context_a": results_a,
                "context_b": results_b,
                "context_c": results_c
            }
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nüìä Detailed results saved to: {output_file}")


if __name__ == "__main__":
    try:
        print("\n" + "="*70)
        print("  IMPORTANT NOTE")
        print("="*70)
        print("\nThis script uses the TRUE OpenAI Responses API:")
        print("  - Endpoint: POST /v1/responses")
        print("  - Method: client.responses.create()")
        print("  - SDK requirement: openai >= 1.50.0 (estimated)")
        print("\nIf you encounter SDK errors, this demonstrates the")
        print("CORRECT way to use Responses API for future reference.\n")
        
        run_experiment()
        
        print("\n" + "="*70)
        print("  API COMPARISON SUMMARY")
        print("="*70)
        print("\nChat Completions API (current mainstream):")
        print("  client.chat.completions.create(messages=[...])")
        print("  ‚Üí response.choices[0].message.content")
        print("\nResponses API (future direction):")
        print("  client.responses.create(input='...')")
        print("  ‚Üí response.output_text")
        print("\n‚Üí Responses API is simpler and more powerful! ‚ú®")
        
    except Exception as e:
        print(f"\n‚ùå Experiment failed: {str(e)}")
        import traceback
        traceback.print_exc()
