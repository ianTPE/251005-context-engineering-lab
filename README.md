# Context Engineering Lab ğŸ§ª

A minimal viable experiment to validate how different **context engineering** approaches affect LLM output quality and consistency.

## ğŸ¯ Experiment Overview

**Task**: Extract structured JSON from product reviews  
**Goal**: Compare three context engineering strategies (A/B/C testing)  
**Duration**: ~10-15 minutes  
**Model**: GPT-4o-mini (configurable)

### The Three Context Strategies

1. **Context A - Baseline** ğŸ“
   - Minimal instruction
   - No format specification
   - No examples
   
2. **Context B - Rules-based** ğŸ“‹
   - Strict output format
   - Explicit constraints
   - Schema definition
   
3. **Context C - Few-shot** ğŸ“
   - Rules from Context B
   - Plus 2 concrete examples
   - Input â†’ Output pairs

## ğŸ†• New: Context Visualization Tools

**Visualize how your context evolves - like `git diff` for prompts!**

Track context changes, compare strategies, and measure real impact with our new visualization toolkit:

### âœ¨ Features

- ğŸ“Š **Context Diff** - See exactly what changed between versions
- ğŸ“ˆ **Evolution Timeline** - Track token usage and improvements
- ğŸ”„ **Side-by-Side Comparison** - Visual comparison of contexts
- ğŸ¯ **Response Quality Metrics** - Measure actual API performance
- ğŸ’¾ **Export Results** - JSON export for analysis

### ğŸš€ Quick Demo

```bash
# Install visualization dependencies
pip install rich tiktoken

# Run demo (no API needed)
python context_visualizer.py

# Run live experiment (requires API key)
python context_visualizer_live.py
```

### ğŸ“š Documentation

- ğŸ¨ [5-Minute Quick Start](./QUICKSTART_VISUALIZATION.md) - Get started immediately
- ğŸ“– [Complete Usage Guide](./VISUALIZATION_GUIDE.md) - All features explained
- ğŸ’¡ [Concepts & Examples](./VISUALIZATION_SUMMARY.md) - Understanding context engineering

### ğŸ¯ What You'll See

```
ğŸ“ˆ Context Evolution Timeline

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step   â”‚ Name        â”‚ Tokens â”‚ Î” Tokens â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ #0     â”‚ Baseline    â”‚     13 â”‚          â”‚
â”‚ #1     â”‚ + Rules     â”‚     65 â”‚      +52 â”‚
â”‚ #2     â”‚ + Examples  â”‚    161 â”‚      +96 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ Response Comparison

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Context     â”‚ Score â”‚ Result      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Baseline    â”‚   0%  â”‚ âŒ Failed   â”‚
â”‚ + Rules     â”‚ 100%  â”‚ âœ… Perfect  â”‚
â”‚ + Examples  â”‚ 100%  â”‚ âœ… Perfect  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Real experiment results show Rules-based prompts achieve 100% accuracy vs 0% for baseline!**

---

## ğŸš€ Quick Start (Original Experiments)

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Set Your OpenAI API Key

**Option A: Environment Variable (PowerShell)**
```powershell
$env:OPENAI_API_KEY='sk-your-key-here'
```

**Option B: Environment Variable (CMD)**
```cmd
set OPENAI_API_KEY=sk-your-key-here
```

**Option C: Create .env file**
```bash
# Copy the example and edit
copy .env.example .env
# Then edit .env with your actual API key
```

### 3. Run the Experiment

**Standard version:**
```bash
python context_experiment.py
```

**With .env support:**
```bash
python context_experiment_dotenv.py
```

**Responses API version (recommended):**
```bash
python context_experiment_responses_api.py
```

## ğŸ“Š What You'll See

The script will:

1. **Test each context** against 3 product reviews (mixed Chinese/English)
2. **Score outputs** based on:
   - Valid JSON parsing âœ“
   - Correct keys present âœ“
   - Valid sentiment values âœ“
   - Non-empty product field âœ“
   
3. **Generate comparison report**:
   ```
   Context A (Baseline)       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          50.0%
   Context B (Rules)          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    80.0%
   Context C (Few-shot)       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100.0%
   ```

4. **Save detailed results** to `experiment_results_TIMESTAMP.json`

## ğŸ” Expected Findings

You should observe:

- âœ… **Baseline â†’ Rules**: Format consistency improves
- âœ… **Rules â†’ Few-shot**: Output reliability increases
- âœ… **Few-shot**: Highest success rate (often 100%)

This validates that **structured context** (rules + examples) produces more reliable, consistent outputs.

## ğŸ“ Test Sentences

The experiment uses these 3 mixed-language product reviews:

1. ğŸ§ "é€™æ”¯è€³æ©ŸéŸ³è³ªä¸éŒ¯ï¼Œä½†è—ç‰™å¸¸å¸¸æ–·ç·šã€‚" (Chinese)
2. âŒ¨ï¸ "The keyboard feels great, but the battery dies too fast." (English)
3. ğŸ“· "ç›¸æ©Ÿç•«è³ªå¾ˆæ£’ï¼Œå¯æ˜¯å¤œæ‹å°ç„¦å¾ˆæ…¢ã€‚" (Chinese)

Expected output schema:
```json
{
  "sentiment": "positive|neutral|negative",
  "product": "product_name",
  "issue": "description_or_empty_string"
}
```

## ğŸ›ï¸ Customization

### Change the Model

Edit `context_experiment.py`:
```python
def call_model(system_prompt, user_message, model="gpt-4o-mini"):
    # Change to: "gpt-4o", "gpt-4-turbo", etc.
```

### Add More Test Cases

Edit the `TESTS` list:
```python
TESTS = [
    "Your new test sentence 1",
    "Your new test sentence 2",
    # ...
]
```

### Adjust Temperature

Lower = more deterministic, Higher = more creative
```python
temperature=0.3  # Range: 0.0 - 2.0
```

## ğŸ“ˆ Next Steps

Want to extend this experiment? Try:

1. **ğŸ†• Context Visualization** (Recommended!)
   - Use `context_visualizer.py` to see context evolution
   - Run `context_visualizer_live.py` for real API testing
   - Compare your custom prompts visually
   - See [QUICKSTART_VISUALIZATION.md](./QUICKSTART_VISUALIZATION.md)

2. **Prompt Injection Testing**
   - Add adversarial test cases
   - Test context robustness
   - Use visualization tools to debug failures
   
3. **Context Slicing**
   - Use longer reviews (500+ words)
   - Test summarization before extraction
   - Track token usage with visualization tools
   
4. **Observability**
   - Add Langfuse/PromptLayer integration
   - Combine with context visualization for complete tracking
   - Track token usage and latency
   
5. **LangGraph Version**
   - Convert A/B/C into state graph nodes
   - Add multi-strategy comparison logic
   - Visualize state transitions

## ğŸ“š Learn More

- [OpenAI Chat Completions API](https://platform.openai.com/docs/guides/chat)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)

## ğŸ› Troubleshooting

### "OPENAI_API_KEY not set"
Make sure you've set the environment variable or created a `.env` file.

### "Insufficient quota"
Check your OpenAI account has available credits at [platform.openai.com/usage](https://platform.openai.com/usage)

### JSON parsing errors
The script automatically handles markdown code blocks. If issues persist, check the raw output in the JSON results file.

### Rate limits
Add a small delay between API calls if you hit rate limits:
```python
import time
time.sleep(1)  # After each API call
```

## ğŸ“„ License

MIT License - Feel free to use and modify for your own experiments!

## ğŸ™‹ Questions?

This is a minimal learning experiment. Feel free to:
- Modify the code
- Add your own test cases
- Experiment with different models
- Share your findings!

---

**Happy Context Engineering! ğŸš€**
