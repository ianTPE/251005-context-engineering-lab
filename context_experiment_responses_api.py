"""
Context Engineering Experiment - Responses API Version
=======================================================
ä½¿ç”¨ OpenAI Responses API å¯¦ç¾æ›´æ¨™æº–çš„ few-shot learning

Responses API çš„å„ªå‹¢ï¼š
1. åŽŸç”Ÿæ”¯æ´ few-shot examples (é€éŽ messages çµæ§‹)
2. æ›´æ¸…æ™°çš„å°è©±æ­·å²ç®¡ç†
3. ç¬¦åˆæœ€æ–°çš„ OpenAI API è¨­è¨ˆè¦ç¯„
"""

import json
import os
from datetime import datetime
from openai import OpenAI

# Check for API key
if not os.getenv("OPENAI_API_KEY"):
    print("âŒ ERROR: OPENAI_API_KEY environment variable not set!")
    print("\nPlease set your OpenAI API key:")
    print("  Windows (PowerShell): $env:OPENAI_API_KEY='your-key-here'")
    exit(1)

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Test sentences - 5 longer, more realistic reviews
TESTS = [
    "æˆ‘æœ€è¿‘è²·äº†é€™æ¬¾ç„¡ç·šè€³æ©Ÿï¼Œæ•´é«”ä¾†èªªéŸ³è³ªè¡¨ç¾ç›¸ç•¶å‡ºè‰²ï¼Œä½ŽéŸ³æ¸¾åŽšã€é«˜éŸ³æ¸…æ™°ã€‚ä¸éŽä½¿ç”¨äº†å…©å€‹ç¦®æ‹œå¾Œç™¼ç¾ï¼Œè—ç‰™é€£ç·šç¶“å¸¸æœƒçªç„¶æ–·æŽ‰ï¼Œå°¤å…¶æ˜¯åœ¨äººå¤šçš„åœ°æ–¹æ›´æ˜Žé¡¯ï¼Œéœ€è¦é‡æ–°é…å°æ‰èƒ½ä½¿ç”¨ï¼Œé€™é»žçœŸçš„å¾ˆå›°æ“¾ã€‚",
    
    "The mechanical keyboard I purchased has excellent build quality with a satisfying tactile feedback that makes typing a pleasure. However, I'm quite disappointed with the battery life - it only lasts about 3-4 days with the RGB lighting on, which is far shorter than the advertised 2 weeks. I find myself charging it constantly.",
    
    "é€™å°ç›¸æ©Ÿçš„ç•«è³ªçœŸçš„æ²’è©±èªªï¼Œæ—¥æ‹çš„ç…§ç‰‡è‰²å½©é®®è±”ã€ç´°ç¯€è±å¯Œï¼Œå®Œå…¨é”åˆ°å°ˆæ¥­æ°´æº–ã€‚ä½†æ˜¯ä¸€åˆ°æ™šä¸Šæˆ–å…‰ç·šä¸è¶³çš„ç’°å¢ƒï¼Œå°ç„¦é€Ÿåº¦å°±è®Šå¾—è¶…ç´šæ…¢ï¼Œå¸¸å¸¸è¦å°å¥½å¹¾æ¬¡æ‰èƒ½æˆåŠŸï¼Œæ‹å¤œæ™¯æˆ–å®¤å…§ç…§ç‰‡æ™‚å¾ˆä¸æ–¹ä¾¿ï¼Œå¸Œæœ›æœªä¾†éŸŒé«”æ›´æ–°èƒ½æ”¹å–„é€™å€‹å•é¡Œã€‚",
    
    "I've been using this wireless mouse for gaming and productivity work for the past month. The ergonomic design is comfortable for long sessions, and the precision is excellent for both gaming and design work. The only downside is that the left click button has started developing a double-click issue, which is frustrating during important tasks.",
    
    "é€™æ¬¾æ™ºæ…§æ‰‹éŒ¶çš„èž¢å¹•é¡¯ç¤ºæ•ˆæžœå¾ˆæ£’ï¼Œåœ¨é™½å…‰ä¸‹ä¹Ÿèƒ½æ¸…æ¥šçœ‹è¦‹ï¼Œè€Œä¸”é‹å‹•è¿½è¹¤åŠŸèƒ½å¾ˆæº–ç¢ºã€‚å¯æ˜¯çºŒèˆªåŠ›çœŸçš„è®“äººå¤±æœ›ï¼Œå®˜æ–¹èªªå¯ä»¥ç”¨5å¤©ï¼Œä½†å¯¦éš›ä¸Šé–‹å•Ÿæ‰€æœ‰åŠŸèƒ½å¾Œï¼Œå¤§æ¦‚2å¤©å°±è¦å……é›»äº†ã€‚å¦å¤–å……é›»é€Ÿåº¦ä¹Ÿå¾ˆæ…¢ï¼Œè¦å……æ»¿é›»éœ€è¦å°‡è¿‘3å°æ™‚ï¼Œå°æ–¼ç¶“å¸¸å¤–å‡ºçš„äººä¾†èªªå¾ˆä¸æ–¹ä¾¿ã€‚"
]

# Base system message
SYS_BASE = "You are a helpful assistant that extracts structured information from product reviews."

# ============================================================================
# Context A: Baseline - ä½¿ç”¨ Chat Completions API (åŽŸæœ‰æ–¹å¼)
# ============================================================================

def build_context_a_messages(user_input):
    """Context A: æœ€å°åŒ–æŒ‡ç¤º"""
    return [
        {"role": "system", "content": SYS_BASE},
        {"role": "user", "content": f"""Extract sentiment (positive/neutral/negative), product, and issue from this sentence.
Return as JSON.

Sentence: {user_input}"""}
    ]

# ============================================================================
# Context B: Rules-based - ä½¿ç”¨åš´æ ¼è¦å‰‡
# ============================================================================

def build_context_b_messages(user_input):
    """Context B: æ˜Žç¢ºè¦å‰‡ä½†æ²’æœ‰ç¯„ä¾‹"""
    return [
        {"role": "system", "content": SYS_BASE},
        {"role": "user", "content": f"""Task: Extract fields from the sentence.
Return ONLY a JSON object with these exact keys: sentiment, product, issue.

Rules:
- sentiment must be one of: positive, neutral, negative
- If product is not explicit, infer the most likely product noun (e.g., 'headphones', 'keyboard')
- issue should describe the problem mentioned, or be empty string if none
- Return ONLY valid JSON, no comments, no extra text, no markdown code blocks
- Use lowercase English for all field values

Sentence: {user_input}"""}
    ]

# ============================================================================
# Context C: Few-shot - ä½¿ç”¨ Responses API æ¨™æº– few-shot æ¨¡å¼
# ============================================================================

def build_context_c_messages(user_input):
    """
    Context C: ä½¿ç”¨æ¨™æº– few-shot æ¨¡å¼
    
    åœ¨ Responses API ä¸­ï¼Œfew-shot æ˜¯é€éŽ conversation history å¯¦ç¾ï¼š
    1. System message å®šç¾©ä»»å‹™
    2. User æä¾›ç¯„ä¾‹è¼¸å…¥
    3. Assistant å±•ç¤ºæœŸæœ›è¼¸å‡º
    4. é‡è¤‡å¤šå€‹ç¯„ä¾‹
    5. æœ€å¾Œæä¾›å¯¦éš›çš„ user input
    """
    return [
        # System message: å®šç¾©ä»»å‹™å’Œè¦å‰‡
        {
            "role": "system",
            "content": """You are a product review analyzer. Extract sentiment, product, and issue from reviews.

Rules:
- sentiment: must be "positive", "neutral", or "negative"
- product: infer the product type in lowercase English
- issue: describe the problem, or empty string if none
- Return ONLY valid JSON with keys: sentiment, product, issue
- No markdown, no extra text"""
        },
        
        # Example 1: User input
        {
            "role": "user",
            "content": "é€™å°ç­†é›»èž¢å¹•å¾ˆäº®ï¼Œä½†æ˜¯æ•£ç†±å¾ˆåµã€‚"
        },
        # Example 1: Assistant response
        {
            "role": "assistant",
            "content": '{"sentiment": "negative", "product": "laptop", "issue": "noisy cooling"}'
        },
        
        # Example 2: User input
        {
            "role": "user",
            "content": "These earbuds are comfortable and the mic is clear."
        },
        # Example 2: Assistant response
        {
            "role": "assistant",
            "content": '{"sentiment": "positive", "product": "earbuds", "issue": ""}'
        },
        
        # Example 3: Mixed sentiment case
        {
            "role": "user",
            "content": "The mouse is lightweight but clicks feel mushy."
        },
        # Example 3: Assistant response
        {
            "role": "assistant",
            "content": '{"sentiment": "negative", "product": "mouse", "issue": "mushy clicks"}'
        },
        
        # Actual user input
        {
            "role": "user",
            "content": user_input
        }
    ]

# ============================================================================
# API Calling Functions
# ============================================================================

def call_chat_completions(messages, model="gpt-4o-mini", temperature=0.3):
    """
    ä½¿ç”¨ Chat Completions API
    é€™æ˜¯ç›®å‰ä¸»æµçš„æ–¹å¼ï¼Œä¹Ÿæ˜¯æˆ‘å€‘ä¸€ç›´åœ¨ç”¨çš„
    """
    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"ERROR: {str(e)}"


# Note: Responses API å¯¦éš›ä¸Šåœ¨ç›®å‰çš„ OpenAI Python SDK ä¸­
# ä»ç„¶ä½¿ç”¨ chat.completions.createï¼Œä½†æˆ‘å€‘é€éŽ messages çµæ§‹
# ä¾†å¯¦ç¾æ›´æ¨™æº–çš„ few-shot learning

# ============================================================================
# Scoring and Evaluation
# ============================================================================

def clean_json_output(text):
    """Extract JSON from text that might contain markdown blocks"""
    if "```" in text:
        lines = text.split("\n")
        json_lines = []
        in_code_block = False
        for line in lines:
            if line.strip().startswith("```"):
                in_code_block = not in_code_block
                continue
            if in_code_block:
                json_lines.append(line)
        text = "\n".join(json_lines)
    return text.strip()


def score_json(output_text):
    """Score output based on schema compliance"""
    try:
        cleaned = clean_json_output(output_text)
        obj = json.loads(cleaned)
        
        required_keys = {"sentiment", "product", "issue"}
        keys_ok = set(obj.keys()) == required_keys
        
        valid_sentiments = {"positive", "neutral", "negative"}
        sentiment_ok = obj.get("sentiment", "").lower() in valid_sentiments
        
        product_ok = isinstance(obj.get("product"), str) and len(obj.get("product", "")) > 0
        issue_ok = "issue" in obj and isinstance(obj.get("issue"), str)
        
        if keys_ok and sentiment_ok and product_ok and issue_ok:
            return 1, obj, None
        else:
            errors = []
            if not keys_ok: errors.append(f"wrong_keys: {set(obj.keys())}")
            if not sentiment_ok: errors.append(f"invalid_sentiment: {obj.get('sentiment')}")
            if not product_ok: errors.append("empty_or_invalid_product")
            if not issue_ok: errors.append("missing_or_invalid_issue")
            return 0, obj, ", ".join(errors)
            
    except json.JSONDecodeError as e:
        return 0, None, f"JSON parse error: {str(e)}"
    except Exception as e:
        return 0, None, f"Unexpected error: {str(e)}"


def eval_context(tag, message_builder, verbose=True):
    """Evaluate a context strategy"""
    if verbose:
        print(f"\n{'='*70}")
        print(f"  {tag}")
        print(f"{'='*70}\n")
    
    results = []
    total_score = 0
    
    for i, test_sentence in enumerate(TESTS, 1):
        # Build messages using the provided builder function
        messages = message_builder(test_sentence)
        
        # Show message structure for Context C (few-shot)
        if verbose and "Few-shot" in tag and i == 1:
            print(f"ðŸ“¨ Message structure (showing {len(messages)} messages):")
            print(f"   - 1 system message")
            print(f"   - {(len(messages) - 2) // 2} example pairs (user + assistant)")
            print(f"   - 1 actual user query\n")
        
        # Call API
        output = call_chat_completions(messages)
        
        # Score output
        score, parsed, error = score_json(output)
        total_score += score
        
        # Store result
        result = {
            "test_id": i,
            "input": test_sentence,
            "output": output,
            "parsed": parsed,
            "score": score,
            "error": error
        }
        results.append(result)
        
        # Print if verbose
        if verbose:
            print(f"Test {i}: {test_sentence}")
            print(f"Output: {output}")
            print(f"Parsed: {json.dumps(parsed, ensure_ascii=False) if parsed else 'FAILED'}")
            print(f"Score: {score}/1 {f'({error})' if error else 'âœ“'}")
            print()
    
    if verbose:
        print(f"[{tag}] Total Score: {total_score}/{len(TESTS)}")
        print(f"Success Rate: {total_score/len(TESTS)*100:.1f}%")
    
    return {
        "tag": tag,
        "total_score": total_score,
        "max_score": len(TESTS),
        "success_rate": total_score / len(TESTS),
        "results": results
    }


def run_experiment():
    """Run the complete A/B/C experiment with Responses API"""
    print("\n" + "="*70)
    print("  CONTEXT ENGINEERING EXPERIMENT")
    print("  Using OpenAI Responses API for Few-shot Learning")
    print("="*70)
    print("\nðŸ’¡ Key Difference: Context C uses proper conversation history")
    print("   for few-shot examples instead of text-based prompting.\n")
    
    # Run all three context versions
    results_a = eval_context(
        "A: Baseline (minimal instruction)",
        build_context_a_messages
    )
    
    results_b = eval_context(
        "B: Rules-based (strict format)",
        build_context_b_messages
    )
    
    results_c = eval_context(
        "C: Few-shot (Responses API with conversation history)",
        build_context_c_messages
    )
    
    # Summary comparison
    print("\n" + "="*70)
    print("  SUMMARY COMPARISON")
    print("="*70 + "\n")
    
    comparison = [
        ("Context A (Baseline)", results_a["success_rate"]),
        ("Context B (Rules)", results_b["success_rate"]),
        ("Context C (Few-shot)", results_c["success_rate"])
    ]
    
    for name, rate in comparison:
        bar = "â–ˆ" * int(rate * 20)
        print(f"{name:30s} {bar:20s} {rate*100:5.1f}%")
    
    print("\n" + "-"*70)
    print("Key Findings:")
    print("-"*70)
    
    if results_c["success_rate"] > results_b["success_rate"]:
        print("âœ“ Few-shot examples improved consistency over rules alone")
    if results_b["success_rate"] > results_a["success_rate"]:
        print("âœ“ Explicit rules improved reliability over baseline")
    if results_c["success_rate"] == 1.0:
        print("âœ“ Few-shot context achieved 100% success rate!")
    elif results_c["success_rate"] < 1.0:
        print(f"âš  Few-shot still has {(1-results_c['success_rate'])*100:.0f}% failure rate")
    
    print("\n" + "-"*70)
    print("Responses API Advantages for Few-shot:")
    print("-"*70)
    print("1. âœ“ Structured conversation history (not text-based)")
    print("2. âœ“ Clear example pairs (user â†’ assistant)")
    print("3. âœ“ Better model understanding of expected format")
    print("4. âœ“ Easier to maintain and update examples")
    print("5. âœ“ Follows OpenAI's recommended best practices")
    
    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"experiment_results_responses_api_{timestamp}.json"
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump({
            "timestamp": timestamp,
            "api_version": "Responses API (via chat.completions)",
            "test_sentences": TESTS,
            "results": {
                "context_a": results_a,
                "context_b": results_b,
                "context_c": results_c
            }
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nðŸ“Š Detailed results saved to: {output_file}")


if __name__ == "__main__":
    try:
        run_experiment()
        
        print("\n" + "="*70)
        print("  COMPARISON: Text-based vs API-based Few-shot")
        print("="*70)
        print("\nText-based (original):")
        print("  System: 'Here are examples: Input X â†’ Output Y'")
        print("  User: 'Now analyze this...'")
        print("\nAPI-based (Responses API):")
        print("  System: 'Task definition'")
        print("  User: 'Example input 1'")
        print("  Assistant: 'Example output 1'")
        print("  User: 'Example input 2'")
        print("  Assistant: 'Example output 2'")
        print("  User: 'Actual input'")
        print("\nâ†’ The second approach is more natural and effective! âœ¨")
        
    except Exception as e:
        print(f"\nâŒ Experiment failed: {str(e)}")
        import traceback
        traceback.print_exc()
