"""
Live Context Visualizer with Real API
æ•´åˆçœŸå¯¦ OpenAI API çš„ Context å¯è¦–åŒ–å·¥å…·

é€™å€‹å·¥å…·æœƒï¼š
1. çœŸå¯¦èª¿ç”¨ OpenAI API
2. è¿½è¹¤æ¯æ¬¡ context è®ŠåŒ–
3. å³æ™‚é¡¯ç¤º diff å’Œæ¼”è®Š
4. æ¯”è¼ƒä¸åŒç­–ç•¥çš„å¯¦éš›æ•ˆæœ
"""

import os
import json
from datetime import datetime
from openai import OpenAI
from dotenv import load_dotenv
from context_visualizer import ContextVisualizer

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# åˆå§‹åŒ– OpenAI client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# æ¸¬è©¦æ¡ˆä¾‹
TESTS = [
    "é€™æ”¯è€³æ©ŸéŸ³è³ªä¸éŒ¯ï¼Œä½†è—ç‰™å¸¸å¸¸æ–·ç·šã€‚",
    "The keyboard feels great, but the battery dies too fast.",
    "ç›¸æ©Ÿç•«è³ªå¾ˆæ£’ï¼Œå¯æ˜¯å¤œæ‹å°ç„¦å¾ˆæ…¢ã€‚"
]


def call_model(system_prompt, user_message, model="gpt-4o-mini"):
    """èª¿ç”¨ OpenAI API"""
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            temperature=0.3
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error: {str(e)}"


def score_response(response: str) -> float:
    """ç°¡å–®çš„è©•åˆ†å‡½æ•¸"""
    score = 0.0
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰æ•ˆ JSON
    try:
        data = json.loads(response)
        score += 0.25
        
        # æª¢æŸ¥å¿…è¦æ¬„ä½
        if "sentiment" in data:
            score += 0.25
        if "product" in data:
            score += 0.25
        if "issue" in data:
            score += 0.25
        
        # æª¢æŸ¥ sentiment å€¼æ˜¯å¦åˆæ³•
        if data.get("sentiment") in ["positive", "neutral", "negative"]:
            score += 0.25
        
        # é¡å¤–åˆ†ï¼šproduct éç©º
        if data.get("product") and data.get("product").strip():
            score += 0.25
        
    except:
        pass
    
    return min(score, 1.0)


def run_live_experiment():
    """åŸ·è¡ŒçœŸå¯¦çš„ API å¯¦é©—ä¸¦å¯è¦–åŒ–"""
    
    # åˆå§‹åŒ–å¯è¦–åŒ–å™¨
    viz = ContextVisualizer()
    
    # å®šç¾©ä¸‰ç¨® Context
    CTX_A = """You are a sentiment analyzer.
Extract product info from this review."""
    
    CTX_B = """You are a sentiment analyzer.

Extract the following information from product reviews:
- sentiment: must be "positive", "neutral", or "negative"
- product: the product name (string)
- issue: description of any issues (string, or empty)

Output must be valid JSON format.
Do not include markdown code blocks."""
    
    CTX_C = CTX_B + """

Examples:

Input: "é€™æ”¯è€³æ©ŸéŸ³è³ªä¸éŒ¯ï¼Œä½†è—ç‰™å¸¸å¸¸æ–·ç·šã€‚"
Output: {"sentiment": "negative", "product": "headphones", "issue": "bluetooth connection"}

Input: "The keyboard feels great, but the battery dies too fast."
Output: {"sentiment": "negative", "product": "keyboard", "issue": "battery life"}"""
    
    # æ·»åŠ  snapshots
    print("\nğŸ“¸ Capturing context snapshots...\n")
    viz.add_snapshot("Context A (Baseline)", CTX_A, {"strategy": "baseline"})
    viz.add_snapshot("Context B (Rules-based)", CTX_B, {"strategy": "rules"})
    viz.add_snapshot("Context C (Few-shot)", CTX_C, {"strategy": "fewshot"})
    
    # é¡¯ç¤ºæ¼”è®Š
    viz.show_evolution()
    
    # é¡¯ç¤º Context A vs B å·®ç•°
    print("\n" + "="*80)
    viz.show_diff(0, 1)
    
    # é¡¯ç¤º Context B vs C å·®ç•°
    print("\n" + "="*80)
    viz.show_diff(1, 2)
    
    # æ¸¬è©¦æ¯å€‹ context
    contexts = [
        ("Context A (Baseline)", CTX_A),
        ("Context B (Rules-based)", CTX_B),
        ("Context C (Few-shot)", CTX_C)
    ]
    
    print("\n" + "="*80)
    print("\nğŸ§ª Running live experiments with OpenAI API...\n")
    
    results = {}
    
    for ctx_name, ctx_content in contexts:
        print(f"\n Testing {ctx_name}...")
        
        scores = []
        responses_text = []
        
        for i, test in enumerate(TESTS, 1):
            print(f"  Test {i}/3...", end=" ")
            
            # èª¿ç”¨ API
            response = call_model(ctx_content, test)
            
            # æ¸…ç† markdown code blocks
            if response.startswith("```"):
                lines = response.split("\n")
                response = "\n".join(lines[1:-1])
                if response.startswith("json"):
                    response = response[4:].strip()
            
            # è©•åˆ†
            test_score = score_response(response)
            scores.append(test_score)
            responses_text.append(response)
            
            print(f"Score: {test_score:.1%}")
        
        # è¨ˆç®—å¹³å‡åˆ†
        avg_score = sum(scores) / len(scores)
        results[ctx_name] = {
            "scores": scores,
            "avg_score": avg_score,
            "responses": responses_text
        }
        
        # è¨˜éŒ„åˆ°å¯è¦–åŒ–å™¨ï¼ˆä½¿ç”¨ç¬¬ä¸€å€‹å›æ‡‰ä½œç‚ºä»£è¡¨ï¼‰
        viz.add_response(ctx_name, responses_text[0], avg_score)
        
        print(f"  âœ… Average Score: {avg_score:.1%}")
    
    # é¡¯ç¤ºä¸¦æ’æ¯”è¼ƒ
    print("\n" + "="*80)
    viz.show_side_by_side(0, 2, max_lines=15)
    
    # é¡¯ç¤ºç›¸ä¼¼åº¦
    print("\n" + "="*80)
    print("\n[Similarity Analysis]")
    viz.show_similarity(0, 2)
    
    # é¡¯ç¤ºå›æ‡‰æ¯”è¼ƒ
    viz.show_response_comparison()
    
    # è©³ç´°çµæœè¡¨æ ¼
    print("\n" + "="*80)
    print("\nğŸ“Š Detailed Results:\n")
    
    for ctx_name, result in results.items():
        print(f"{ctx_name}:")
        for i, (test, score) in enumerate(zip(TESTS, result["scores"]), 1):
            print(f"  Test {i}: {score:.1%} | {test[:40]}...")
        print(f"  Average: {result['avg_score']:.1%}\n")
    
    # å°å‡º
    viz.export_comparison()
    
    # ä¿å­˜è©³ç´°çµæœ
    detailed_filename = f"live_experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(detailed_filename, 'w', encoding='utf-8') as f:
        json.dump({
            "timestamp": datetime.now().isoformat(),
            "tests": TESTS,
            "results": results
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… Detailed results saved to {detailed_filename}")
    
    return results


if __name__ == "__main__":
    print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘  Live Context Engineering Visualizer                    â•‘")
    print("â•‘  Real-time API calls + Visualization                    â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    # æª¢æŸ¥ API key
    if not os.getenv("OPENAI_API_KEY"):
        print("\nâŒ Error: OPENAI_API_KEY not set")
        print("Please set it in .env file or environment variable")
    else:
        results = run_live_experiment()
        
        print("\n" + "="*80)
        print("\nğŸ‰ Experiment completed! Check the exported files for details.")
