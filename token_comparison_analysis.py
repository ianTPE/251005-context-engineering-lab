"""
Token Usage Analysis: Rules-based vs Few-shot
==============================================
åˆ†æå…©ç¨®promptç­–ç•¥çš„tokenä½¿ç”¨é‡å·®ç•°ï¼Œå¹«åŠ©é¸æ“‡æœ€ç¶“æ¿Ÿçš„æ–¹æ³•
"""

import tiktoken

def build_context_b_input(user_sentence):
    """Context B: Rules-based"""
    return f"""Task: Extract fields from the sentence.
Return ONLY a JSON object with these exact keys: sentiment, product, issue.

Rules:
- sentiment must be one of: positive, neutral, negative
- If product is not explicit, infer the most likely product noun (e.g., 'headphones', 'keyboard')
- issue should describe the problem mentioned, or be empty string if none
- Return ONLY valid JSON, no comments, no extra text, no markdown code blocks
- Use lowercase English for all field values

Sentence: {user_sentence}"""


def build_context_c_input(user_sentence):
    """Context C: Few-shot"""
    return f"""You are a product review analyzer. Extract sentiment, product, and issue from reviews.

Rules:
- sentiment: must be "positive", "neutral", or "negative"
- product: infer the product type in lowercase English
- issue: describe the problem, or empty string if none
- Return ONLY valid JSON with keys: sentiment, product, issue
- No markdown, no extra text

Examples:

Example 1:
Input: "é€™å°ç­†é›»è¢å¹•å¾ˆäº®ï¼Œä½†æ˜¯æ•£ç†±å¾ˆåµã€‚"
Output: {{"sentiment": "negative", "product": "laptop", "issue": "noisy cooling"}}

Example 2:
Input: "These earbuds are comfortable and the mic is clear."
Output: {{"sentiment": "positive", "product": "earbuds", "issue": ""}}

Example 3:
Input: "The mouse is lightweight but clicks feel mushy."
Output: {{"sentiment": "negative", "product": "mouse", "issue": "mushy clicks"}}

Now analyze this sentence:
Input: "{user_sentence}"
Output:"""


def count_tokens(text, model="gpt-4"):
    """è¨ˆç®—æ–‡æœ¬çš„tokenæ•¸é‡"""
    try:
        encoding = tiktoken.encoding_for_model(model)
        return len(encoding.encode(text))
    except KeyError:
        # å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨cl100k_baseç·¨ç¢¼ï¼ˆGPT-4ç³»åˆ—é€šç”¨ï¼‰
        encoding = tiktoken.get_encoding("cl100k_base")
        return len(encoding.encode(text))


def analyze_token_usage():
    """åˆ†æå…©ç¨®æ–¹æ³•çš„tokenä½¿ç”¨é‡"""
    
    # æ¸¬è©¦å¥å­æ¨£æœ¬ï¼ˆä¸åŒé•·åº¦ï¼‰
    test_sentences = [
        "Good product",  # çŸ­å¥
        "é€™æ”¯è€³æ©ŸéŸ³è³ªä¸éŒ¯ï¼Œä½†è—ç‰™å¸¸å¸¸æ–·ç·šã€‚",  # ä¸­ç­‰é•·åº¦
        "I've been using this wireless mouse for gaming and productivity work for the past month. The ergonomic design is comfortable for long sessions, and the precision is excellent for both gaming and design work. The only downside is that the left click button has started developing a double-click issue, which is frustrating during important tasks.",  # é•·å¥
    ]
    
    print("=" * 80)
    print("  TOKEN USAGE ANALYSIS: Rules-based vs Few-shot")
    print("=" * 80)
    
    total_rules_tokens = 0
    total_fewshot_tokens = 0
    
    for i, sentence in enumerate(test_sentences, 1):
        print(f"\nğŸ“ Test {i}: {sentence[:50]}{'...' if len(sentence) > 50 else ''}")
        print("-" * 70)
        
        # Rules-basedæ–¹æ³•
        rules_input = build_context_b_input(sentence)
        rules_tokens = count_tokens(rules_input)
        
        # Few-shotæ–¹æ³•
        fewshot_input = build_context_c_input(sentence)
        fewshot_tokens = count_tokens(fewshot_input)
        
        # è¨ˆç®—å·®ç•°
        diff = fewshot_tokens - rules_tokens
        diff_percent = (diff / rules_tokens) * 100
        
        print(f"Rules-based tokens:  {rules_tokens:4d}")
        print(f"Few-shot tokens:     {fewshot_tokens:4d}")
        print(f"Difference:          {diff:+4d} ({diff_percent:+5.1f}%)")
        
        total_rules_tokens += rules_tokens
        total_fewshot_tokens += fewshot_tokens
    
    # ç¸½è¨ˆ
    total_diff = total_fewshot_tokens - total_rules_tokens
    avg_diff_percent = (total_diff / total_rules_tokens) * 100
    
    print("\n" + "=" * 70)
    print("  SUMMARY")
    print("=" * 70)
    print(f"Total Rules-based tokens:  {total_rules_tokens:5d}")
    print(f"Total Few-shot tokens:     {total_fewshot_tokens:5d}")
    print(f"Total difference:          {total_diff:+5d} ({avg_diff_percent:+5.1f}%)")
    
    print(f"\nğŸ’° Cost implications (å‡è¨­ GPT-4 å®šåƒ¹):")
    cost_per_1k_tokens = 0.03  # å‡è¨­åƒ¹æ ¼
    rules_cost = (total_rules_tokens / 1000) * cost_per_1k_tokens
    fewshot_cost = (total_fewshot_tokens / 1000) * cost_per_1k_tokens
    cost_diff = fewshot_cost - rules_cost
    
    print(f"   Rules-based cost:  ${rules_cost:.4f}")
    print(f"   Few-shot cost:     ${fewshot_cost:.4f}")
    print(f"   Cost difference:   ${cost_diff:+.4f}")
    
    return {
        "rules_tokens": total_rules_tokens,
        "fewshot_tokens": total_fewshot_tokens,
        "difference": total_diff,
        "difference_percent": avg_diff_percent
    }


def provide_selection_strategy():
    """æä¾›é¸æ“‡ç­–ç•¥å»ºè­°"""
    print("\n" + "=" * 70)
    print("  SELECTION STRATEGY RECOMMENDATIONS")
    print("=" * 70)
    
    print("""
ğŸ¯ Tokenå„ªåŒ–é¸æ“‡ç­–ç•¥ï¼š

1. ğŸ“Š **æª¢æ¸¬éšæ®µ**ï¼š
   - å…ˆç”¨å°‘é‡æ¨£æœ¬æ¸¬è©¦å…©ç¨®æ–¹æ³•çš„æº–ç¢ºç‡
   - å¦‚æœæº–ç¢ºç‡ç›¸åŒï¼Œé¸æ“‡tokenæ›´å°‘çš„æ–¹æ³•
   
2. ğŸ”€ **å‹•æ…‹é¸æ“‡**ï¼š
   ```python
   def choose_prompt_strategy(task_complexity, budget_priority):
       if budget_priority == "high":
           return "rules_based"  # é€šå¸¸æ›´çœtoken
       elif task_complexity == "high":
           return "few_shot"     # æ›´ç©©å®šçš„è¡¨ç¾
       else:
           return "rules_based"  # é è¨­é¸æ“‡
   ```

3. ğŸ“ **åŸºæ–¼è¼¸å…¥é•·åº¦**ï¼š
   - çŸ­å¥(<50å­—): Rules-based (å·®ç•°ä¸å¤§)
   - é•·å¥(>100å­—): è€ƒæ…®Few-shotçš„ç©©å®šæ€§vsæˆæœ¬
   
4. ğŸ§ª **A/Bæ¸¬è©¦å»ºè­°**ï¼š
   - åœ¨å°‘é‡æ¨£æœ¬ä¸Šå¿«é€Ÿæ¸¬è©¦
   - æ¸¬é‡æº–ç¢ºç‡ + tokenä½¿ç”¨é‡
   - è¨ˆç®—ROI: accuracy_gain / token_cost_increase
   
5. ğŸ’¡ **æ··åˆç­–ç•¥**ï¼š
   - ç°¡å–®æ¡ˆä¾‹: Rules-based
   - è¤‡é›œ/é‚Šç·£æ¡ˆä¾‹: Few-shot
   - å‹•æ…‹åˆ‡æ›é–€æª»: å¤±æ•—2æ¬¡â†’å‡ç´šåˆ°Few-shot
""")

def create_token_aware_selector():
    """å‰µå»ºè€ƒæ…®tokenæˆæœ¬çš„é¸æ“‡å™¨"""
    print("""
ğŸ› ï¸ **å¯¦ç”¨ç¨‹å¼ç¢¼ç¯„ä¾‹**ï¼š

```python
def smart_prompt_selector(user_input, accuracy_threshold=0.95):
    \"\"\"
    æ™ºèƒ½é¸æ“‡promptç­–ç•¥
    \"\"\"
    # 1. ä¼°ç®—å…©ç¨®æ–¹æ³•çš„tokenä½¿ç”¨é‡
    rules_tokens = estimate_rules_tokens(user_input)
    fewshot_tokens = estimate_fewshot_tokens(user_input)
    
    # 2. å¦‚æœtokenå·®ç•°å°(<20%)ï¼Œå„ªå…ˆé¸Rules-based
    token_diff_percent = (fewshot_tokens - rules_tokens) / rules_tokens
    if token_diff_percent < 0.2:
        return "rules_based"
    
    # 3. å¦‚æœå·®ç•°å¤§ï¼Œéœ€è¦è€ƒæ…®æº–ç¢ºç‡æå‡æ˜¯å¦å€¼å¾—
    # (é€™éœ€è¦åŸºæ–¼æ­·å²è³‡æ–™çš„æ©Ÿå™¨å­¸ç¿’æ¨¡å‹)
    estimated_accuracy_gain = predict_accuracy_gain(user_input)
    cost_benefit_ratio = estimated_accuracy_gain / token_diff_percent
    
    if cost_benefit_ratio > 2.0:  # æº–ç¢ºç‡æå‡è¶…éæˆæœ¬å¢åŠ 2å€
        return "few_shot"
    else:
        return "rules_based"
```
""")


if __name__ == "__main__":
    # åŸ·è¡Œtokenåˆ†æ
    results = analyze_token_usage()
    
    # æä¾›é¸æ“‡ç­–ç•¥
    provide_selection_strategy()
    
    # æä¾›å¯¦ç”¨å·¥å…·
    create_token_aware_selector()
    
    print(f"\nâœ… Analysis complete! Few-shot uses {results['difference_percent']:+.1f}% more tokens than rules-based.")